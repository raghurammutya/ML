# Backpressure Quick Reference Guide

## üö® **IMMEDIATE ACTIONS (Before Deploying New Code)**

These are tactical strategies you can apply **RIGHT NOW** to detect and mitigate backpressure with your current setup.

---

## 1Ô∏è‚É£ **DETECTION: Check Current Health**

### **A. Check Redis Pub/Sub Stats**

```bash
# Connect to Redis
redis-cli

# Check active clients
CLIENT LIST

# Check pub/sub channels
PUBSUB CHANNELS ticker:*
PUBSUB NUMSUB ticker:options:256265 ticker:underlying:NIFTY

# Check memory usage
INFO memory

# Check slowlog (publishes taking > 10ms)
SLOWLOG GET 10
```

**Red Flags:**
- ‚ùå Many clients with `age > 300s` (stale connections)
- ‚ùå `used_memory` > 80% of `maxmemory`
- ‚ùå `mem_fragmentation_ratio` > 1.5
- ‚ùå Slowlog entries for `PUBLISH` commands

### **B. Monitor Application Logs**

```bash
# Check for Redis errors
docker logs ticker-service | grep -i redis | grep -i error

# Check for timeout warnings
docker logs ticker-service | grep -i timeout

# Check publish latency
docker logs ticker-service | grep "Published to" | tail -20
```

**Red Flags:**
- ‚ùå `Redis publish failed` messages
- ‚ùå `Connection timeout` errors
- ‚ùå Publish latency > 100ms consistently

### **C. Check System Resources**

```bash
# Check ticker service container stats
docker stats ticker-service --no-stream

# Check Redis container stats
docker stats redis --no-stream
```

**Red Flags:**
- ‚ùå CPU > 80% sustained
- ‚ùå Memory growth over time (leak)
- ‚ùå Network I/O saturated

---

## 2Ô∏è‚É£ **TACTICAL MITIGATION (No Code Changes)**

### **Strategy A: Reduce Subscription Count**

**Impact:** Immediate 50-80% reduction in data volume

```bash
# Check current subscriptions
curl http://localhost:8080/subscriptions | jq '. | length'

# Identify high-volume instruments
curl http://localhost:8080/subscriptions | jq '.[] | .tradingsymbol'

# Delete subscriptions for far OTM options
curl -X DELETE http://localhost:8080/subscriptions/{instrument_token}
```

**Recommendations:**
- Keep only ATM ¬± 5 strikes (reduce from ¬±10)
- Remove weekly expiries, keep only monthly
- Unsubscribe deep ITM/OTM options (delta < 0.1 or > 0.9)

### **Strategy B: Optimize Redis Configuration**

**Impact:** 30-50% latency reduction

```bash
# Edit redis.conf
sudo vim /etc/redis/redis.conf
```

**Add/Update:**
```conf
# Increase network throughput
tcp-backlog 511
tcp-nodelay yes

# Optimize memory
maxmemory 4gb
maxmemory-policy allkeys-lru

# Disable persistence (if data loss is acceptable)
save ""
appendonly no

# Increase client limits
maxclients 10000
```

```bash
# Restart Redis
docker restart redis
```

### **Strategy C: Add Redis Connection Pooling**

**Impact:** 20-30% latency reduction

Current code uses single connection. Add connection pooling:

```python
# Quick fix in redis_client.py
client = redis.from_url(
    self._settings.redis_url,
    decode_responses=True,
    max_connections=20,  # Add connection pool
    socket_keepalive=True,
    socket_keepalive_options={
        socket.TCP_KEEPIDLE: 60,
        socket.TCP_KEEPINTVL: 10,
        socket.TCP_KEEPCNT: 3
    }
)
```

### **Strategy D: Rate Limit Ingestion**

**Impact:** Prevents overwhelming downstream

Add to `generator.py` in `_handle_ticks()`:

```python
async def _handle_ticks(self, account_id: str, lookup: Dict, ticks: List[Dict]):
    # Rate limit: process max 100 ticks per call
    if len(ticks) > 100:
        logger.warning(f"Received {len(ticks)} ticks, sampling to 100")
        import random
        ticks = random.sample(ticks, 100)

    # ... rest of existing code
```

### **Strategy E: Add Publish Timeout**

**Impact:** Prevents hanging on slow Redis

Quick fix in `redis_client.py`:

```python
async def publish(self, channel: str, message: str) -> None:
    # ... existing code

    try:
        # Add timeout wrapper
        await asyncio.wait_for(
            self._client.publish(channel, message),
            timeout=1.0  # Max 1 second
        )
    except asyncio.TimeoutError:
        logger.error(f"Redis publish timeout on {channel}")
        # Don't retry, just log and continue
        return
```

---

## 3Ô∏è‚É£ **MONITORING: Set Up Alerts**

### **A. Simple Health Check Script**

Save as `/tmp/check_backpressure.sh`:

```bash
#!/bin/bash

# Check ticker service health
HEALTH=$(curl -s http://localhost:8080/health)
STATUS=$(echo $HEALTH | jq -r '.status')

if [ "$STATUS" != "ok" ]; then
    echo "‚ùå ALERT: Ticker service unhealthy: $STATUS"
    echo $HEALTH | jq '.'
else
    echo "‚úÖ Ticker service healthy"
fi

# Check Redis memory
REDIS_MEMORY=$(redis-cli INFO memory | grep used_memory_human)
echo "Redis Memory: $REDIS_MEMORY"

# Check pending publishes (if using v2 publisher)
PENDING=$(curl -s http://localhost:8080/advanced/backpressure/status 2>/dev/null | jq -r '.pending_publishes')
if [ ! -z "$PENDING" ] && [ "$PENDING" -gt 100 ]; then
    echo "‚ö†Ô∏è  WARNING: High pending publishes: $PENDING"
fi
```

```bash
# Run every minute
chmod +x /tmp/check_backpressure.sh
watch -n 60 /tmp/check_backpressure.sh
```

### **B. Log Monitoring with Grep**

```bash
# Monitor logs in real-time for issues
docker logs -f ticker-service 2>&1 | grep -E "(error|timeout|failed|warning)" --color=always
```

### **C. Redis Monitoring**

```bash
# Monitor Redis in real-time
redis-cli --stat
redis-cli --latency
redis-cli --latency-history
```

---

## 4Ô∏è‚É£ **EMERGENCY RESPONSE PLAYBOOK**

### **Scenario A: Redis Connection Timeout**

**Symptoms:**
- Logs show "Redis publish failed"
- Circuit breaker state = "open"
- High latency (> 500ms)

**Actions:**
```bash
# 1. Check Redis is running
docker ps | grep redis

# 2. Check Redis logs
docker logs redis | tail -50

# 3. Check Redis CPU/memory
docker stats redis --no-stream

# 4. If Redis overloaded, restart
docker restart redis

# 5. Reload ticker service subscriptions
curl -X POST http://localhost:8080/admin/instrument-refresh
```

### **Scenario B: High Memory Usage**

**Symptoms:**
- Memory usage > 2GB and growing
- OOM (out of memory) errors

**Actions:**
```bash
# 1. Check memory breakdown
docker stats ticker-service --no-stream

# 2. Reduce subscriptions immediately
curl http://localhost:8080/subscriptions | jq '.[100:] | .[].instrument_token' | \
  xargs -I {} curl -X DELETE http://localhost:8080/subscriptions/{}

# 3. Restart service if memory > 3GB
docker restart ticker-service

# 4. Consider horizontal scaling
docker-compose up -d --scale ticker-service=2
```

### **Scenario C: High CPU Usage**

**Symptoms:**
- CPU > 80% sustained
- Slow responses
- High latency

**Actions:**
```bash
# 1. Profile the service
docker exec ticker-service pip install py-spy
docker exec ticker-service py-spy top --pid 1 --duration 30

# 2. Check for tight loops
docker logs ticker-service | grep "Published.*mock" | wc -l

# 3. Reduce tick frequency
# Edit config: stream_interval_seconds from 1.0 to 2.0

# 4. Scale horizontally
docker-compose up -d --scale ticker-service=2
```

### **Scenario D: Consumer Lag (Frontend Not Keeping Up)**

**Symptoms:**
- Redis memory growing
- Frontend shows stale data
- WebSocket messages queued

**Actions:**
```bash
# 1. Check Redis pub/sub queue depth
redis-cli CLIENT LIST | grep -E "(age|qbuf)"

# 2. Identify slow consumers
redis-cli CLIENT LIST | awk '$NF > 1000 {print $0}'

# 3. Kill slow clients
redis-cli CLIENT KILL ADDR <ip:port>

# 4. Frontend: Implement message sampling
# (see BACKPRESSURE_STRATEGY.md section 4.E)
```

---

## 5Ô∏è‚É£ **CAPACITY PLANNING**

### **Current System Limits (Estimated)**

| Metric | Current Capacity | Bottleneck |
|--------|------------------|------------|
| Max subscriptions | ~500 instruments | Kite API limit |
| Max tick rate | ~1000 ticks/sec | Redis pub/sub |
| Max concurrent clients | ~100 WebSocket | Ticker service CPU |
| Redis pub/sub throughput | ~50K msg/sec | Redis single-threaded |
| Network bandwidth | ~10 Mbps | Docker network |

### **Scaling Strategies**

**Vertical Scaling (Single Instance):**
- ‚úÖ Easy to implement
- ‚úÖ No code changes
- ‚ùå Limited to ~2000 ticks/sec
- ‚ùå Single point of failure

```bash
# Increase Docker resources
docker update ticker-service --cpus="4" --memory="4g"
```

**Horizontal Scaling (Multiple Instances):**
- ‚úÖ Unlimited throughput
- ‚úÖ High availability
- ‚ùå Requires load balancer
- ‚ùå Session affinity needed

```bash
# Scale to 3 instances
docker-compose up -d --scale ticker-service=3
```

**Redis Scaling:**
- ‚úÖ Horizontal pub/sub with sharding
- ‚úÖ High availability with Sentinel
- ‚ùå Complex setup
- ‚ùå Requires code changes

```bash
# Use Redis Cluster (6 nodes: 3 masters + 3 replicas)
redis://redis-m1:6379,redis-m2:6379,redis-m3:6379
```

---

## 6Ô∏è‚É£ **QUICK WINS (Implementation Priority)**

### **Week 1: Immediate Fixes (No New Code)**
1. ‚úÖ Reduce subscriptions to ATM ¬± 5 strikes
2. ‚úÖ Optimize Redis config (disable persistence)
3. ‚úÖ Add publish timeout (1 line change)
4. ‚úÖ Set up monitoring script

**Expected Impact:** 50-70% latency reduction

### **Week 2: Deploy Backpressure Monitoring**
1. ‚úÖ Deploy `backpressure_monitor.py`
2. ‚úÖ Add API endpoints
3. ‚úÖ Set up Grafana dashboard
4. ‚úÖ Configure alerts

**Expected Impact:** Visibility into system health

### **Week 3: Deploy Resilient Publisher**
1. ‚úÖ Deploy `redis_publisher_v2.py`
2. ‚úÖ Migrate high-volume channels
3. ‚úÖ Test failure scenarios
4. ‚úÖ Monitor metrics

**Expected Impact:** 5x throughput, 10x latency, auto-recovery

### **Week 4: Optimization & Tuning**
1. ‚úÖ Tune buffer sizes based on metrics
2. ‚úÖ Adjust sampling thresholds
3. ‚úÖ Implement consumer-side backpressure
4. ‚úÖ Load testing

**Expected Impact:** Production-grade reliability

---

## 7Ô∏è‚É£ **TESTING BACKPRESSURE SCENARIOS**

### **Simulate Redis Overload**

```bash
# 1. Start ticker service normally
docker-compose up -d

# 2. Stress test Redis
redis-cli DEBUG SLEEP 10  # Blocks Redis for 10 seconds

# 3. Observe behavior
# - Circuit breaker should open
# - Messages should buffer
# - After 10s, circuit should auto-recover
```

### **Simulate High Ingestion Rate**

```bash
# 1. Subscribe to many instruments (> 500)
for i in {1..1000}; do
  curl -X POST http://localhost:8080/subscriptions \
    -H "Content-Type: application/json" \
    -d "{\"instrument_token\": $((256265 + i)), \"requested_mode\": \"FULL\"}"
done

# 2. Monitor backpressure metrics
watch -n 1 'curl -s http://localhost:8080/advanced/backpressure/status | jq .'

# 3. Observe adaptive sampling kick in
# - WARNING: 80% sampling
# - CRITICAL: 50% sampling
# - OVERLOAD: 20% sampling + load shedding
```

### **Simulate Consumer Lag**

```bash
# 1. Subscribe to Redis but don't consume messages
redis-cli SUBSCRIBE "ticker:*"
# Don't read messages, let them queue

# 2. Monitor Redis memory growth
watch -n 1 'redis-cli INFO memory | grep used_memory_human'

# 3. Kill slow subscriber
redis-cli CLIENT LIST | grep -E "age=[5-9][0-9]+"
redis-cli CLIENT KILL ADDR <ip:port>
```

---

## 8Ô∏è‚É£ **USEFUL COMMANDS CHEAT SHEET**

```bash
# === Health Checks ===
curl http://localhost:8080/health | jq '.'
curl http://localhost:8080/advanced/backpressure/status | jq '.'
redis-cli PING

# === Metrics ===
curl http://localhost:8080/metrics | grep ticker_
curl http://localhost:8080/advanced/backpressure/metrics | jq '.'

# === Subscriptions ===
curl http://localhost:8080/subscriptions | jq '. | length'
curl -X DELETE http://localhost:8080/subscriptions/{token}

# === Redis ===
redis-cli INFO stats | grep instantaneous
redis-cli CLIENT LIST
redis-cli PUBSUB CHANNELS "ticker:*"
redis-cli SLOWLOG GET 10

# === Docker ===
docker logs ticker-service --tail 100 -f
docker stats ticker-service redis --no-stream
docker restart ticker-service

# === Circuit Breaker ===
curl http://localhost:8080/advanced/backpressure/circuit-breaker | jq '.'
curl -X POST http://localhost:8080/advanced/backpressure/reset-circuit-breaker \
  -H "X-API-Key: your-key"
```

---

## üìä **SUCCESS CRITERIA**

Your backpressure management is working when:

‚úÖ **Ingestion rate ‚âà Publish rate** (ratio > 95%)
‚úÖ **P99 latency < 100ms**
‚úÖ **Backpressure level = HEALTHY** (or WARNING briefly during spikes)
‚úÖ **No dropped messages** (or < 1% during CRITICAL states)
‚úÖ **Circuit breaker state = CLOSED**
‚úÖ **Redis memory stable** (not growing over time)
‚úÖ **CPU < 60%** average
‚úÖ **Zero Redis timeout errors** in logs

---

## üÜò **WHEN TO ESCALATE**

Escalate to infrastructure team if:

‚ùå **Sustained OVERLOAD** state for > 5 minutes
‚ùå **Circuit breaker OPEN** for > 2 minutes
‚ùå **Drop rate > 50%** consistently
‚ùå **Redis memory > 90%**
‚ùå **OOM (out of memory) crashes**
‚ùå **Network saturation** (> 80% bandwidth)
‚ùå **Cascading failures** (multiple services affected)

---

**Last Updated:** 2025-10-31
**Version:** 1.0
**Author:** Claude
